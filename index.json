[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. student in the Computer Science department at the University of California, San Diego working with Prof. Jingbo Shang. I am broadly interested in Machine Learning and Natural Language Processing.\nFor summer 2022, I am interning at Microsoft Semantic Machines with Dr. Subhro Roy working on Weakly Supervised Semantic Parsing. I completed my Bachelor Of Technology in Computer Science And Engineering from the Indian Institute of Technology, Kanpur in 2017, where I worked with Prof. Harish Karnick and Prof. Purushottam Kar. I worked as a Data Scientist and Product Engineer at Sprinklr for 2 years and I interned at Microsoft India in the summer of 2016.\nCurrent Research Few Shot \u0026amp; Weakly Supervised Learning I develop high performing deep neural frameworks with minimal human supervision such as just class labels or a few label-indicative seed words. I am interested in leveraging massive amounts of unstructured and unlabeled data available on the internet for supervision and additional contextual information. Further, I am also keen on beneficially leveraging pre-trained language models to reduce the need for annotated data.\nSecurity I study vulnerabilities of current NLP systems such as data poisoning and trigger-based backdoor attacks and work towards developing strong defense methods against such attacks.\nDeep Learning A deep neural network is known to learn/overfit any randomly labeled data. I am interested in understanding and unveiling the learning process of deep neural architectures and further use it to analyze the quality of data.\nApart from Academics, I enjoy spending time playing Ukulele, playing Football(soccer) and I rarely write too. Checkout my blog!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dheeraj7596.github.io/author/dheeraj-mekala/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dheeraj-mekala/","section":"authors","summary":"I am a Ph.D. student in the Computer Science department at the University of California, San Diego working with Prof. Jingbo Shang. I am broadly interested in Machine Learning and Natural Language Processing.","tags":null,"title":"Dheeraj Mekala","type":"authors"},{"authors":["Dheeraj Mekala","Jason Wolfe","Subhro Roy"],"categories":null,"content":"","date":1671580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671580800,"objectID":"d6f2ee8a5454e0ecbf236666ea9eb110","permalink":"https://dheeraj7596.github.io/publication/zerotop/","publishdate":"2022-12-21T00:00:00Z","relpermalink":"/publication/zerotop/","section":"publication","summary":"We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes a semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems, enabling us to leverage the ability of LLMs to zero-shot answer reading comprehension questions. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. To address this problem, we fine-tune a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can correctly parse ≈ 16% of utterances in the MTOP dataset without requiring any annotated data.","tags":null,"title":"ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models","type":"publication"},{"authors":["Dheeraj Mekala","Tu Vu","Timo Schick","Jingbo Shang"],"categories":null,"content":"","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653436800,"objectID":"ec305d59cc146d838be616c991c41164","permalink":"https://dheeraj7596.github.io/publication/conda/","publishdate":"2022-05-25T00:00:00Z","relpermalink":"/publication/conda/","section":"publication","summary":"Manually annotating datasets requires domain experts to read through many documents and carefully label them, which is often expensive. Recently, pre-trained generative language models (GLMs) have demonstrated exceptional abilities in generating text which motivates to leverage them for generative data augmentation. We improve generative data augmentation by formulating the data generation as context generation task and use question answering (QA) datasets for intermediate training. Specifically, we view QA to be more as a format than of a task and train GLMs as context generators for a given question and its respective answer. Then, we cast downstream tasks into question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which is further used as synthetic training data for their corresponding tasks. We perform extensive experiments, case studies, and ablation studies on multiple sentiment and topic classification datasets and demonstrate substantial improvements in performance in few-shot, zero-shot settings. Remarkably, on the SST2 dataset, intermediate training on SocialIQA dataset achieves an improvement of 40% on Macro-F1 score. Through thorough analyses, we observe that QA datasets that requires highlevel reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.","tags":null,"title":"Leveraging QA Datasets to Improve Generative Data Augmentation","type":"publication"},{"authors":["Dheeraj Mekala","Chengyu Dong","Jingbo Shang"],"categories":null,"content":"","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653436800,"objectID":"54c695991f81fe8749bc9ed9a2e251f4","permalink":"https://dheeraj7596.github.io/publication/lops/","publishdate":"2022-05-25T00:00:00Z","relpermalink":"/publication/lops/","section":"publication","summary":"Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets.","tags":null,"title":"LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification","type":"publication"},{"authors":["Sudhanshu Ranjan","Dheeraj Mekala","Jingbo Shang"],"categories":null,"content":"","date":1653350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653350400,"objectID":"856d004dbed732861667cd118439c9c2","permalink":"https://dheeraj7596.github.io/publication/progressive/","publishdate":"2022-05-24T00:00:00Z","relpermalink":"/publication/progressive/","section":"publication","summary":"Multilingual transformer language models have recently attracted much attention from researchers and are used in cross-lingual transfer learning for many NLP tasks such as text classification and named entity recognition. However, similar methods for transfer learning from monolingual text to code-switched text have not been extensively explored mainly due to the following challenges: (1) Codeswitched corpus, unlike monolingual corpus, consists of more than one language and existing methods can’t be applied efficiently, (2) Code-switched corpus is usually made of resource-rich and low-resource languages and upon using multilingual pre-trained language models, the final model might bias towards resource-rich language. In this paper, we focus on code-switched sentiment analysis where we have a labelled resource-rich language dataset and unlabelled code-switched data. We propose a framework that takes the distinction between resource-rich and low-resource language into account. Instead of training on the entire code-switched corpus at once, we create buckets based on the fraction of words in the resource-rich language and progressively train from resource-rich language dominated samples to low-resource language dominated samples. Extensive experiments across multiple language pairs demonstrate that progressive training helps low-resource language dominated samples.","tags":null,"title":"Progressive Sentiment Analysis for Code-Switched Text Data","type":"publication"},{"authors":["Zichao Li","Dheeraj Mekala","Chengyu Dong","Jingbo Shang"],"categories":null,"content":"","date":1631404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631404800,"objectID":"aa9b696d3d0d9465277bcf92e9d01052","permalink":"https://dheeraj7596.github.io/publication/bfclass/","publishdate":"2021-09-12T00:00:00Z","relpermalink":"/publication/bfclass/","section":"publication","summary":"Backdoor attack introduces artificial vulnerabilities into the model by poisoning a subset of the training data via injecting triggers and modifying labels. Various trigger design strategies have been explored to attack text classifiers, however, defending such attacks remains an open problem. In this work, we propose BFClass, a novel efficient backdoor-free training framework for text classification. The backbone of BFClass is a pre-trained discriminator that predicts whether each token in the corrupted input was replaced by a masked language model. To identify triggers, we utilize this discriminator to locate the most suspicious token from each training sample and then distill a concise set by considering their association strengths with particular labels. To recognize the poisoned subset, we examine the training samples with these identified triggers as the most suspicious token, and check if removing the trigger will change the poisoned model’s prediction. Extensive experiments demonstrate that BFClass can identify all the triggers, remove 95% poisoned training samples with very limited false alarms, and achieve almost the same performance as the models trained on the benign training data.","tags":null,"title":"BFClass: A Backdoor-free Text Classification Framework","type":"publication"},{"authors":["Dheeraj Mekala","Varun Gangal","Jingbo Shang"],"categories":null,"content":"","date":1631404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631404800,"objectID":"ec278d99f113e2bf1c2c51256c6f52da","permalink":"https://dheeraj7596.github.io/publication/c2f/","publishdate":"2021-09-12T00:00:00Z","relpermalink":"/publication/c2f/","section":"publication","summary":"Existing text classification methods mainly focus on a fixed label set, whereas many real-world applications require extending to new fine-grained classes as the number of samples per label increases. To accommodate such requirements, we introduce a new problem called coarse-to-fine grained classification, which aims to perform fine-grained classification on coarsely annotated data. Instead of asking for new fine-grained human annotations, we opt to leverage label surface names as the only human guidance and weave in rich pre-trained generative language models into the iterative weak supervision strategy. Specifically, we first propose a label-conditioned fine-gtuning formulation to attune these generators for our task. Furthermore, we devise a regularization objective based on the coarse-fine label constraints derived from our problem setting, giving us even further improvements over the prior formulation. Our framework uses the fine-tuned generative models to sample pseudo-training data for training the classifier, and bootstraps on real unlabeled data for model refinement. Extensive experiments and case studies on two real-world datasets demonstrate superior performance over SOTA zero-shot classification baselines.","tags":null,"title":"Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data","type":"publication"},{"authors":[],"categories":[],"content":"Ad-hoc Document Retrieval using WeakSupervision with BERT and GPT2 ","date":1619548705,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619548705,"objectID":"f415122149049d1b10d593b0dcbfb327","permalink":"https://dheeraj7596.github.io/post/mar_2021_nlp/","publishdate":"2021-04-27T11:38:25-07:00","relpermalink":"/post/mar_2021_nlp/","section":"post","summary":"The blog post summarizing a few papers from EMNLP 2020 and some recent papers that I have enjoyed reading in March 2021.","tags":[],"title":"March 2021 NLP Reading: ","type":"post"},{"authors":["Zihan Wang","Dheeraj Mekala","Jingbo Shang"],"categories":null,"content":"","date":1615420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615420800,"objectID":"daf1a7b57635ad7cda1447c3592f0583","permalink":"https://dheeraj7596.github.io/publication/xclass/","publishdate":"2021-03-11T00:00:00Z","relpermalink":"/publication/xclass/","section":"publication","summary":"In this paper, we explore to conduct text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective -- ideal document representations should lead to very close results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations must be adaptive to the given class names. We propose a novel framework X-Class to realize it. Specifically, we first estimate comprehensive class representations by incrementally adding the most similar word to each class until inconsistency appears. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized token representations. We then cluster and align the documents to classes with the prior of each document assigned to its nearest class. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets.","tags":null,"title":"X-Class: Text Classification with Extremely Weak Supervision","type":"publication"},{"authors":["Dheeraj Mekala","Xinyang Zhang","Jingbo Shang"],"categories":null,"content":"","date":1600214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600214400,"objectID":"b2321277f07d924a90b2bdec0728ae1b","permalink":"https://dheeraj7596.github.io/publication/meta/","publishdate":"2020-09-16T00:00:00Z","relpermalink":"/publication/meta/","section":"publication","summary":"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as “seed motifs”, which provide additional weak supervision. Following a bootstrapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.","tags":null,"title":"META: Metadata-Empowered Weak Supervision for Text Classification","type":"publication"},{"authors":["Dheeraj Mekala","Jingbo Shang"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8d08b4d40973ce39b689e8b6008f4b35","permalink":"https://dheeraj7596.github.io/publication/conwea/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/conwea/","section":"publication","summary":"Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.","tags":null,"title":"Contextualized Weak Supervision for Text Classification","type":"publication"},{"authors":["Rahul Wadbude","Vivek Gupta","Dheeraj Mekala","Harish Karnick"],"categories":null,"content":"","date":1515283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515283200,"objectID":"3106506114df20382093264727ae04e9","permalink":"https://dheeraj7596.github.io/publication/ubr/","publishdate":"2018-01-07T00:00:00Z","relpermalink":"/publication/ubr/","section":"publication","summary":"Review score prediction of text reviews has recently gained a lot of attention in recommendation systems. A major problem in models for review score prediction is the presence of noise due to user-bias in review scores. We propose two simple statistical methods to remove such noise and improve review score prediction. Compared to other methods that use multiple classifiers, one for each user, our model uses a single global classifier to predict review scores. We empirically evaluate our methods on two major categories (Electronics and Movies and TV) of the SNAP published Amazon e-Commerce Reviews data-set and Amazon Fine Food reviews data-set. We obtain improved review score prediction for three commonly used text feature representations.","tags":null,"title":"User Bias Removal in Review Score Prediction","type":"publication"},{"authors":["Dheeraj Mekala","Vivek Gupta","Bhargavi Paranjape","Harish Karnick"],"categories":null,"content":"","date":1504742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504742400,"objectID":"f4fca25bad8544c7a9afffeabfb7cc69","permalink":"https://dheeraj7596.github.io/publication/scdv/","publishdate":"2017-09-07T00:00:00Z","relpermalink":"/publication/scdv/","section":"publication","summary":"We present a feature vector formation technique for documents - Sparse Composite Document Vector (SCDV) which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In SCDV, word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex, multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks, we outperform the previous state-of-the-art method, NTSG (Liu et al., 2015a). We also show that SCDV embeddings perform well on heterogeneous tasks like Topic Coherence, context-sensitive Learning and Information Retrieval. Moreover, we achieve significant reduction in training and prediction times compared to other representation methods. SCDV achieves best of both worlds - better performance with lower time and space complexity","tags":null,"title":"SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations","type":"publication"},{"authors":["Dheeraj Mekala","Vivek Gupta","Purushottam Kar","Harish Karnick"],"categories":null,"content":"","date":1494115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494115200,"objectID":"1e6410e36bf161434bd2326d47453b6f","permalink":"https://dheeraj7596.github.io/publication/bopt/","publishdate":"2017-05-07T00:00:00Z","relpermalink":"/publication/bopt/","section":"publication","summary":"Hierarchical classification is supervised multi-class classification problem over the set of class labels organized according to a hierarchy. In this project, we study the work by Ramaswamy et al. on hierarchical classification over symmetric tree distance loss. We extend the consistency of hierarchical classification algorithm over asymmetric tree distance loss. We design a O(nk log n) algorithm to find bayes optimal classification for a k-ary tree as hierarchy. We show that under reasonable assumptions over asymmetric loss function, the Bayes optimal classification over this asymmetric loss can be found in O(k log n). We exploit this insight and attempt to extend the Ova-Cascade algorithm Ramaswamy et al. for hierarchical classification over asymmetric loss","tags":null,"title":"Bayes-optimal Hierarchical Classification over Asymmetric Tree-Distance Loss","type":"publication"}]